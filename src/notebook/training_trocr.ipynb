{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import RandomCrop, ToPILImage\n",
    "from torchvision.transforms.functional import crop\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    ")\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContainerOCRDatasetText(Dataset):\n",
    "    def __init__(self, directory, processor, is_train=True):\n",
    "        if directory is None:\n",
    "            print(\"Directory is none\")\n",
    "            return\n",
    "        if processor is None:\n",
    "            print(\"Processor is none\")\n",
    "            return\n",
    "        self.processor = processor\n",
    "        self.directory = Path(directory)\n",
    "        self.image_label = []\n",
    "        if is_train:\n",
    "            self.decode(\n",
    "                file_path=str(self.directory.joinpath(\"train/_annotations.coco.json\")),\n",
    "                is_train=is_train,\n",
    "            )\n",
    "        else:\n",
    "            self.decode(\n",
    "                file_path=str(self.directory.joinpath(\"valid/_annotations.coco.json\")),\n",
    "                is_train=is_train,\n",
    "            )\n",
    "\n",
    "    def decode(self, file_path: str, is_train=True):\n",
    "        with open(file_path) as file:\n",
    "            jsonData = json.load(file)\n",
    "            for image in jsonData[\"images\"]:\n",
    "                image_id = image[\"id\"]\n",
    "                image_filename = image[\"file_name\"]\n",
    "                for annotation in jsonData[\"annotations\"]:\n",
    "                    if annotation[\"image_id\"] == image_id:\n",
    "                        bounding_box = annotation[\"bbox\"]\n",
    "                        x1, y1 = int(bounding_box[0]), int(bounding_box[1])\n",
    "                        x2, y2 = x1 + int(bounding_box[2]), y1 + int(bounding_box[3])\n",
    "                        self.image_label.append(\n",
    "                            {\n",
    "                                \"image_filename\": f'{\"train\" if is_train else \"valid\"}/{image_filename}',\n",
    "                                \"bbox\": [x1, y1, x2, y2],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.decode_image(\n",
    "            str(self.directory.joinpath(f\"{self.image_label[index]['image_filename']}\"))\n",
    "        )\n",
    "        text = self.image_label[index][\"image_filename\"].split(\"_\")[1]\n",
    "        x1, y1, x2, y2 = self.image_label[index][\"bbox\"]\n",
    "        original_image = image[..., y1:y2, x1:x2]\n",
    "        image = self.processor(original_image * 255, return_tensors=\"pt\").pixel_values\n",
    "        labels = self.processor.tokenizer(\n",
    "            text, padding=\"max_length\", max_length=10\n",
    "        ).input_ids\n",
    "        labels = [\n",
    "            label if label != self.processor.tokenizer.pad_token_id else -100\n",
    "            for label in labels\n",
    "        ]\n",
    "        return image[0], torch.tensor(labels), text, original_image\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(image_path):\n",
    "        return read_image(image_path, ImageReadMode.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "ocrModel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "ocrModel = ocrModel.to(device)\n",
    "\n",
    "ocrModel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "ocrModel.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "ocrModel.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "ocrModel.config.max_length = 10\n",
    "ocrModel.config.no_repeat_ngram_size = 3\n",
    "ocrModel.config.length_penalty = 2.0\n",
    "ocrModel.config.num_beams = 4\n",
    "optimizer = torch.optim.AdamW(ocrModel.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 42891, 89, 2], [0, 9178, 32, 47, 2]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer(['hello there', 'how are you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': [array([[[-0.78039217, -0.372549  , -0.00392157, ...,  0.4431373 ,\n",
       "          0.15294123, -0.12156862],\n",
       "        [-0.5529412 , -0.19999999,  0.03529418, ...,  0.27843142,\n",
       "         -0.0745098 , -0.38039213],\n",
       "        [-0.3098039 ,  0.03529418,  0.18431377, ...,  0.18431377,\n",
       "         -0.16862744, -0.45098037],\n",
       "        ...,\n",
       "        [ 0.56078434, -0.09019607, -0.64705884, ...,  0.33333337,\n",
       "          0.39607847,  0.5529412 ],\n",
       "        [ 0.4666667 , -0.18431371, -0.73333335, ..., -0.23137254,\n",
       "          0.09803927,  0.5764706 ],\n",
       "        [ 0.34901965, -0.3098039 , -0.827451  , ..., -0.7490196 ,\n",
       "         -0.17647058,  0.5764706 ]],\n",
       "\n",
       "       [[ 0.7019608 ,  0.16078436, -0.2862745 , ..., -0.70980394,\n",
       "         -0.34117645,  0.07450986],\n",
       "        [-0.09803921, -0.38039213, -0.5764706 , ...,  0.02745104,\n",
       "          0.26274514,  0.48235297],\n",
       "        [-0.827451  , -0.79607844, -0.6862745 , ...,  0.52156866,\n",
       "          0.5764706 ,  0.5921569 ],\n",
       "        ...,\n",
       "        [ 0.3803922 , -0.1607843 , -0.7176471 , ..., -0.4588235 ,\n",
       "         -0.27843136, -0.01176471],\n",
       "        [ 0.3803922 , -0.01960784, -0.5137255 , ...,  0.082353  ,\n",
       "          0.32549024,  0.5686275 ],\n",
       "        [ 0.27058828,  0.06666672, -0.27843136, ...,  0.58431375,\n",
       "          0.8039216 ,  0.9764706 ]],\n",
       "\n",
       "       [[-0.6392157 ,  0.17647064,  0.70980394, ...,  0.12941182,\n",
       "          0.10588241,  0.23921573],\n",
       "        [-0.372549  ,  0.02745104,  0.3411765 , ..., -0.00392157,\n",
       "          0.13725495,  0.3803922 ],\n",
       "        [-0.14509803, -0.06666666,  0.05882359, ..., -0.25490195,\n",
       "          0.04313731,  0.39607847],\n",
       "        ...,\n",
       "        [ 0.58431375,  0.06666672, -0.5372549 , ..., -0.12941176,\n",
       "         -0.11372548, -0.00392157],\n",
       "        [ 0.78039217,  0.082353  , -0.6156863 , ..., -0.25490195,\n",
       "         -0.35686272, -0.3960784 ],\n",
       "        [ 0.85882354, -0.01176471, -0.78039217, ..., -0.29411763,\n",
       "         -0.5686275 , -0.8901961 ]]], dtype=float32), array([[[ 0.5686275 ,  0.427451  ,  0.17647064, ...,  0.60784316,\n",
       "          0.45098042,  0.33333337],\n",
       "        [-0.06666666,  0.12156868,  0.27058828, ...,  0.20784318,\n",
       "          0.17647064,  0.27843142],\n",
       "        [-0.60784316, -0.1607843 ,  0.30196083, ..., -0.17647058,\n",
       "         -0.12941176,  0.09019613],\n",
       "        ...,\n",
       "        [-0.5294118 , -0.47450978, -0.41176468, ..., -0.15294117,\n",
       "         -0.3098039 , -0.34117645],\n",
       "        [-0.60784316, -0.2862745 , -0.09019607, ..., -0.09019607,\n",
       "          0.09019613,  0.27843142],\n",
       "        [-0.4588235 , -0.00392157,  0.21568632, ..., -0.06666666,\n",
       "          0.54509807,  0.9843137 ]],\n",
       "\n",
       "       [[-0.8117647 , -0.7411765 , -0.6862745 , ...,  0.654902  ,\n",
       "          0.6       ,  0.6       ],\n",
       "        [-0.38039213, -0.46666664, -0.5137255 , ..., -0.06666666,\n",
       "          0.28627455,  0.69411767],\n",
       "        [ 0.20000005, -0.10588235, -0.32549018, ..., -0.5529412 ,\n",
       "          0.082353  ,  0.75686276],\n",
       "        ...,\n",
       "        [-0.84313726, -0.3960784 ,  0.13725495, ..., -0.04313725,\n",
       "         -0.10588235, -0.20784312],\n",
       "        [-0.7411765 , -0.16862744,  0.36470592, ...,  0.1686275 ,\n",
       "          0.3411765 ,  0.45098042],\n",
       "        [-0.5294118 ,  0.05882359,  0.48235297, ...,  0.3803922 ,\n",
       "          0.7176471 ,  0.9529412 ]],\n",
       "\n",
       "       [[-0.18431371, -0.38823527, -0.5529412 , ...,  0.33333337,\n",
       "         -0.24705881, -0.7411765 ],\n",
       "        [-0.47450978, -0.40392154, -0.372549  , ..., -0.01960784,\n",
       "         -0.3333333 , -0.64705884],\n",
       "        [-0.7647059 , -0.4352941 , -0.18431371, ..., -0.25490195,\n",
       "         -0.31764704, -0.46666664],\n",
       "        ...,\n",
       "        [-0.6313726 , -0.27843136,  0.16078436, ..., -0.32549018,\n",
       "         -0.5921569 , -0.78039217],\n",
       "        [-0.4352941 , -0.00392157,  0.37254906, ..., -0.14509803,\n",
       "         -0.25490195, -0.27058822],\n",
       "        [ 0.01176476,  0.32549024,  0.427451  , ...,  0.09803927,\n",
       "          0.19215691,  0.3803922 ]]], dtype=float32), array([[[-0.4980392 ,  0.11372554,  0.7254902 , ...,  0.6784314 ,\n",
       "          0.13725495, -0.44313723],\n",
       "        [-0.31764704,  0.15294123,  0.5372549 , ..., -0.05882353,\n",
       "          0.05882359,  0.20784318],\n",
       "        [-0.19215685,  0.13725495,  0.30980396, ..., -0.64705884,\n",
       "          0.0196079 ,  0.7882353 ],\n",
       "        ...,\n",
       "        [ 0.35686278,  0.15294123, -0.06666666, ...,  0.27843142,\n",
       "         -0.09803921, -0.4588235 ],\n",
       "        [-0.08235294, -0.05882353, -0.00392157, ...,  0.48235297,\n",
       "          0.1686275 , -0.24705881],\n",
       "        [-0.46666664, -0.15294117,  0.24705887, ...,  0.5529412 ,\n",
       "          0.48235297,  0.21568632]],\n",
       "\n",
       "       [[ 0.27843142,  0.38823533,  0.34901965, ..., -0.5294118 ,\n",
       "         -0.77254903, -1.        ],\n",
       "        [-0.32549018,  0.07450986,  0.37254906, ..., -0.12156862,\n",
       "         -0.52156866, -0.8901961 ],\n",
       "        [-0.6156863 , -0.15294117,  0.27058828, ...,  0.27058828,\n",
       "         -0.12941176, -0.5058824 ],\n",
       "        ...,\n",
       "        [-0.11372548, -0.41176468, -0.52156866, ...,  0.6784314 ,\n",
       "          0.11372554, -0.5294118 ],\n",
       "        [-0.2235294 , -0.2862745 , -0.2235294 , ...,  0.5137255 ,\n",
       "          0.05882359, -0.5294118 ],\n",
       "        [-0.1372549 ,  0.0196079 ,  0.19215691, ...,  0.20000005,\n",
       "         -0.02745098, -0.38039213]],\n",
       "\n",
       "       [[-0.25490195,  0.33333337,  0.85882354, ..., -0.49019605,\n",
       "          0.09803927,  0.6313726 ],\n",
       "        [-0.11372548,  0.254902  ,  0.62352943, ..., -0.01176471,\n",
       "          0.20000005,  0.33333337],\n",
       "        [-0.03529412,  0.05098045,  0.20000005, ...,  0.3411765 ,\n",
       "          0.23921573,  0.10588241],\n",
       "        ...,\n",
       "        [ 0.5294118 ,  0.6627451 ,  0.78039217, ..., -0.23137254,\n",
       "          0.30196083,  0.8980392 ],\n",
       "        [ 0.7019608 ,  0.78039217,  0.827451  , ..., -0.00392157,\n",
       "          0.10588241,  0.13725495],\n",
       "        [ 0.94509804,  0.90588236,  0.827451  , ...,  0.3176471 ,\n",
       "         -0.08235294, -0.7019608 ]]], dtype=float32)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(torch.randint(size=[3, 3, 256, 256], low=0, high=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in ContainerOCRDatasetText(\"container_dataset/\", processor=processor):\n",
    "    processed_image, image, label = sample[0], sample[-1], sample[1]\n",
    "\n",
    "    if image.size(1) < image.size(2):\n",
    "        continue\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2)\n",
    "    axes[1].imshow(torch.permute(image, [1, 2, 0]))\n",
    "    axes[0].imshow(torch.permute(processed_image, [1, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocrModel.load_state_dict(torch.load('./data/model/33_model.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ContainerOCRDatasetText(\n",
    "    directory=\"./container_dataset/\",\n",
    "    processor=processor,\n",
    "    is_train=True,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    training_data,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_data = ContainerOCRDatasetText(\n",
    "    directory=\"./container_dataset/\",\n",
    "    processor=processor,\n",
    "    is_train=False,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=5,\n",
    ")\n",
    "\n",
    "for sample in train_dataloader:\n",
    "    print(sample[1][0])\n",
    "    break\n",
    "\n",
    "# for sample in train_dataloader:\n",
    "#     plt.figure()\n",
    "#     print(sample[0].size())\n",
    "#     plt.imshow(torch.permute(sample[0].squeeze(), [1, 2, 0]).cpu().numpy())\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(torch.permute(sample[2].squeeze(), [1, 2, 0]).cpu().numpy())\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    sum_cer = 0\n",
    "    for pred, label in zip(pred_ids, label_ids):\n",
    "        pred_str = processor.decode(pred, skip_special_tokens=True)\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "        label_str = processor.decode(label, skip_special_tokens=True)\n",
    "        print(f'\"{label_str}\"  \"{pred_str}\"')\n",
    "\n",
    "        if pred_str == \"\":\n",
    "            sum_cer += len(label_str)\n",
    "        elif label_str == \"\":\n",
    "            sum_cer += len(pred_str)\n",
    "        else:\n",
    "            sum_cer += cer_metric.compute(\n",
    "                predictions=[pred_str], references=[label_str]\n",
    "            )\n",
    "        print(sum_cer)\n",
    "    return sum_cer / len(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 200):\n",
    "    # train_running_loss = 0.0\n",
    "    # ocrModel.train()\n",
    "    # for idx, data in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "    #     inputs: torch.Tensor\n",
    "    #     labels: torch.Tensor\n",
    "    #     inputs, labels = data[0], data[1]\n",
    "\n",
    "    #     inputs = inputs.to(device)\n",
    "    #     labels = labels.to(device)\n",
    "\n",
    "    #     output = ocrModel(inputs, labels=labels)\n",
    "    #     output.loss.backward()\n",
    "    #     train_running_loss += output.loss.item()\n",
    "    #     optimizer.step()\n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    validation_cer = 0\n",
    "    ocrModel.eval()\n",
    "    for data in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            inputs: torch.Tensor\n",
    "            labels: torch.Tensor\n",
    "            inputs, labels = data[0], data[1]\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            generated_ids = ocrModel.generate(inputs)\n",
    "\n",
    "            cer = compute_cer(generated_ids, labels)\n",
    "            validation_cer += cer\n",
    "    print(validation_cer / len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ContainerOCRDatasetText(\n",
    "    directory=\"./container_dataset/\",\n",
    "    processor=processor,\n",
    "    is_train=False,\n",
    ")\n",
    "\n",
    "for idx, sample in enumerate(test_data):\n",
    "    with torch.no_grad():\n",
    "        plt.figure()\n",
    "        plt.imshow(torch.permute(sample[0], dims=[1, 2, 0]))\n",
    "\n",
    "        sample[1][sample[1] == -100] = processor.tokenizer.pad_token_id\n",
    "        generated_ids = ocrModel.generate(sample[0].to(\"mps\").unsqueeze(dim=0))\n",
    "        generated_text = processor.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )[0]\n",
    "        print(generated_text, processor.decode(sample[1], skip_special_tokens=True))\n",
    "\n",
    "    if idx == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
